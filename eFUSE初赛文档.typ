#import "cse-template.typ": *
#import "@preview/codelst:2.0.2": sourcecode
#show: setup-lovelace

#let algorithm = algorithm.with(supplement: "算法")



#show: project.with(
  anonymous: false,
  title: "基于eBPF加速的高性能用户态文件系统",
  abstract_zh: [
  这是一段摘要
  ],
  team_name: "FastPoke",
  team_mates: "许辰涛、冯可逸、赵胜杰",
  school_name: "哈尔滨工业大学（深圳）",
  comp_name: "proj289",
  school_mentor: "夏文、李诗逸",
  comp_mentor: "郑昱笙",
  date: (2025, 6)
)


= 概述

== 背景及意义

FUSE（Filesystem in Userspace）是一种允许在用户态构建文件系统的linux机制，使开发者能够在不必修改内核源码的条件下，便捷且灵活地开发自定义文件系统，极大地降低了开发门槛，简化了开发流程，提高了内核安全性。然而，FUSE 的性能瓶颈一直备受诟病，尤其在高频繁元数据操作、大量小文件读写等场景下，内核态与用户态频繁切换成为主要性能瓶颈，限制了其在特定的高性能场景下的适用性。

在 FUSE 内部的实现中，来自 VFS (虚拟文件系统) 层的所有请求都被放入共享的待处理队列 (pending queue) 中，并由 FUSE 守护进程逐个提取。这种调度方式在某些高并发的场景下会导致严重的锁争用。在多核环境下，无法充分发挥多核处理器的并行优势，使得系统在面对大规模的I/O任务时吞吐率首先，处理时延较高，无法充分利用带宽的潜力。

eBPF（extended Berkeley Packet Filter）是 Linux 的一项强大特性，允许开发者在不修改内核源码的情况下向内核注入用户定义逻辑，已广泛应用于网络、安全、追踪等领域，eBPF 为解决和优化上述 FUSE 的性能问题提供了新的可能和方向。近年来，已有多项研究探索将 eBPF 引入文件系统以提升其性能，例如 ExtFuse、Fuse-BPF、XRP 等。我们期望通过本项目，进一步探索基于 eBPF 的 FUSE 加速路径，实现低延迟、高吞吐、具有良好扩展性的用户态文件系统。

== 目标

eFUSE 是一个尝试将 eBPF 深度集成到 FUSE 文件系统中的创新项目，旨在重构 FUSE 的传统执行路径和请求调度方式，以提高用户态文件系统的运行效率，同时保留 FUSE 的灵活性和安全性的优势。借助 eBPF 这一灵活的特性，对特定的文件系统进行性能优化，实现以下三大目标：


- *减少内核态与用户态之间的频繁切换*：在内核中直接处理部分 FUSE 请求（如 LOOKUP、READ 等），避免传统 FUSE 工作流程中频繁的内核/用户态切换，提高请求处理效率。
- *设计高效的 I/O 和元数据缓存机制*：利用 eBPF 的 map 数据结构实现元数据和读写数据的缓存机制，降低磁盘的访问频率。
- *实现跨核高并发优化与负载均衡机制*：针对 FUSE 共享请求队列带来的并发限制，设计更为合理、更适合多核的请求调度方式，并结合 eBPF 进行负载监控，避免锁的集中争用。



== 行动项

为实现上述目标，进一步将本项目分为五大技术目标模块：

#figure(
  table(
    columns: (100pt, 300pt),
    inset: 10pt,
    stroke: 0.7pt,
    align: left,
    [*实现内容*], [*说明*],
    [目标1：FUSE 内核模块扩展],
    [
      1. 支持新的eBPF程序类型。 
      2. 扩展FUSE挂载点支持。  
      3. 设计并注册文件系统相关 helper 函数。
    ],
    [目标2：FUSE 元数据请求优化],
    [
      1. 优化 inode、目录、权限、路径等相关操作。
      2. 使用 eBPF map 实现元数据缓存。 
      3. 实现内核态与用户态高效协调访问。
      4. 内核/用户态切换次数显著下降。
    ],
    [目标3：FUSE I/O 请求的特殊优化],
    [
      1. 支持直通路径：eBPF 直接读取文件内容。 
      2. 支持缓存路径：将内容存入 eBPF map 缓存。 
      3. 设计请求调度策略实现直通与缓存路径选择
      4. 读写性能提升 1.5\~3 倍。
    ],
    [目标4：基于内核修改的多核优化],
    [
      1. 为每个核心构建独立 ringbuf 管道代替请求队列。
      2. 实现可扩展的核间通信机制。
      3. 实现多核 CPU 环境的适配。
    ],
    [目标5：负载监控与请求均衡],
    [
      1. 利用 eBPF 动态分析请求负载。
      2. 根据 ringbuf 状态进行调度策略调整。
      3. 针对不同的负载情况实现合理的请求分配。
    ]
  ),
  caption: "目标技术模块"
) <tbl1>

我们将上述目标拆分为以下若干行动项：


- 行动项 1：进行背景知识调研，了解 FUSE 的核心性能瓶颈。

- 行动项 2：搭建开发环境。

- 行动项 3：FUSE 内核驱动扩展、加载 eBPF 程序、设置挂载点入口。

- 行动项 4：实现并注册内核 eBPF helper 辅助函数。

- 行动项 5：实现 FUSE 元数据请求绕过路径和回退机制。

- 行动项 6：在用户态和内核中协调访问。

- 行动项 7：实现 FUSE I/O 请求 map 缓存绕过路径。

- 行动项 8：实现 FUSE I/O 请求直通绕过路径。

- 行动项 9：实现 FUSE I/O 请求中的自适应调度算法。

- 行动项10：FUSE 请求绕过机制的安全性评估和处理。

- 行动项11：为 FUSE 内核设计更为合理的请求队列数据结构。

- 行动项12：通过 eBPF 实现对请求队列的负载监控和请求均衡。

- 行动项13：模拟常见的负载场景并进行性能评估。

== 完成情况

在初赛阶段，针对上述行动项的完成情况如下：

- 行动项1 （完成）：讨论并选定可行的 FUSE 优化方向。


- 行动项2 （完成）：在虚拟机中搭建测试环境，基于 linux 6.5 开发。


- 行动项3 （完成）：使指定文件系统在挂载时自动加载 eBPF 程序，完成eBPF程序在送往用户态文件系统时的自动触发。


- 行动项4 （完成）：在内核中设计并注册合适的 eBPF helper 函数，便于后续开发，同时须确保 eBPF 程序安全性。


- 行动项5 （完成）：实现 LOOUP、GETATTR 等元数据请求的绕过机制，大幅降低文件系统在运行时的内核态/用户态切换次数。


- 行动项6 （完成）：对指定的用户态文件系统做一定的修改，使其与 eBPF 程序协调配合，管理 eBPF map 中的数据内容。


- 行动项7 （完成）：实现以 READ、WRITE 为主的文件 I/O 请求的 eBPF map 缓存机制，加快请求的处理速度。


- 行动项8 （完成）：实现以 READ、WRITE 为主的文件 I/O 请求的 eBPF 直通路径，作为对缓存机制的补充。


- 行动项9 （完成）：设计并实现自适应路径选择算法，使系统在不同的负载情况下预测并选择较优的路径，读写性能提升 1.5\~3 倍。


- 行动项10 （完成）：对完成的请求绕过机制进行安全性检查，防止文件读取越界等情况发生，进行处理和优化。


- 行动项11 （完成）:在多核环境下为每个核心分配环形管道，代替原先的请求队列。


- 行动项12 （进行中）


- 行动项13 （基本完成）:设计模拟常见的负载场景测试。

== 开发历程


== 团队分工


#pagebreak()

= 现有研究调研

== FUSE

== eBPF

#pagebreak()

= 整体架构设计

#pagebreak()

= 模块设计和实现

== FUSE 内核模块扩展

== FUSE 元数据请求优化

== FUSE I/O 请求优化

== 多核扩展模块

== 负载监控与请求均衡

#pagebreak()

= 项目测试

== 单核测试

== 多核测试

#pagebreak()

= 总结与展望

#pagebreak()

#bibliography("./ref.bib")
